{"name":"scrapr","version":"0.0.15","description":"A tool for getting public website content using a browser engine or http get.","main":"scrapr.js","repository":"https://github.com/renancaldas/scrapr","scripts":{"test":"echo \"Error: no test specified\" && exit 1"},"license":"ISC","dependencies":{"cheerio":"^0.20.0","debug":"^2.2.0","path":"^0.12.7","q":"^1.4.1","request":"^2.72.0","slimerjs":"^0.906.1"},"keywords":["scrapr","scraper","scraping","crawler","crawling","site","website","web","site","miner","mining","data"],"gitHead":"c4b97cdce9f189b42a52491b72ecdb8ad5a4bc2e","homepage":"https://github.com/renancaldas/scrapr#readme","versions":[{"number":"0.0.1","date":"2016-04-20T23:51:50.200Z"},{"number":"0.0.2","date":"2016-04-20T23:52:18.942Z"},{"number":"0.0.3","date":"2016-04-21T00:27:09.406Z"},{"number":"0.0.4","date":"2016-04-21T01:05:01.593Z"},{"number":"0.0.5","date":"2016-04-21T01:11:01.130Z"},{"number":"0.0.6","date":"2016-04-21T01:42:13.088Z"},{"number":"0.0.7","date":"2016-04-26T12:29:55.635Z"},{"number":"0.0.8","date":"2016-04-27T15:58:03.964Z"},{"number":"0.0.9","date":"2016-04-27T16:17:10.178Z"},{"number":"0.0.10","date":"2016-04-27T16:17:52.684Z"},{"number":"0.0.11","date":"2016-05-04T19:52:10.011Z"},{"number":"0.0.12","date":"2016-05-04T19:57:02.204Z"},{"number":"0.0.13","date":"2016-06-08T04:28:09.371Z"},{"number":"0.0.14","date":"2016-06-08T04:36:51.932Z"},{"number":"0.0.15","date":"2016-07-06T16:11:32.224Z"}],"readme":"# Scrapr\n\n\n##### \"Why should I use this\"?\nThere are websites that rely on javascript frameworks (like jQuery or AngularJS) and process dynamic data after the page load. For these type of websites, you should use a browser to interpert the javascript code and then get the data. Which is what this tool does: provides methods for this task using [SlimerJS](https://slimerjs.org/) (Firefox's  Gecko engine)  in the background.\n\n\n#### Installation\n```sh\n  $ npm install scrapr --save\n```\n\n#### Methods\n\n---\n\n##### getHtmlViaBrowser(url, loadImages)\n * **url**: string (required)\n * **loadImages**: bool (optional)\n\nOpens a browser under the hood, waits for the page load and then gets the data. Returns a promise with a jQuery object ($). This is useful if the page relies on javascript and updates the html content after the page load.\n\n```\nvar scrapr = require('scrapr');\n\n// Opens a browser (loading images), goes to google, gets html tag, finds \"feeling lucky button\" and prints it\nscrapr.getHtmlViaBrowser('http://www.google.com', true).then(function($){\n    $('html').filter(function(){  \n      var htmlTag = $(this);\n      var luckyButton = htmlTag.find('input.lsb')[0];\n      console.log($(luckyButton).attr('value'));\n    });\n}, function(err){\n    console.log('Could not request page. Error: ' + err.message);\n});\n```\n\n---\n\n##### getHtmlViaRequest(url) \n* **url**: string (required)\n\nMakes a direct GET request to the url and returns a promise with a jQuery object ($). This is useful if the page does not rely on javascript and updates the html content after the page load.\n```\nvar scrapr = require('scrapr');\n\n// Gets google's html tag, finds \"feeling lucky button\" and prints it\nscrapr.getHtmlViaRequest('http://www.google.com').then(function($){\n    $('html').filter(function(){  \n        var htmlTag = $(this);\n        var luckyButton = htmlTag.find('input.lsb')[0];\n        console.log($(luckyButton).attr('value'));\n    });\n}, function(err){\n    console.log('Could not request page. Error: ' + err.message);\n});\n```\n\n\n---\n\n##### parseListIntoSlices(arrayToParse, length)\n* **arrayToParse**: string (required)\n* **length**: number (required)\n\nA helper function that splits a large array into slices with the specified length. Useful for throttling large amount of requests while doing parallel requests. For example: scraping 50 pages into slices of 5 with a minute interval for each slice.\n\n```\nvar scrapr = require('scrapr');\n\n// Creates an array of 50 elements and then split it into slices of 6\nvar largeArray = new Array();\nfor(var i = 0; i < 50; i++) {\n  largeArray.push(i);\n}\n\nvar slices = scrapr.parseListIntoSlices(largeArray, 6);\nconsole.log(slices);\n```\n\n#### Author\n\nRenan Caldas de Oliveira\n- Web: http://www.renancaldas.com\n- E-mail: renan.caldas@outlook.com\n- Twitter: https://twitter.com/renanzeirah\n- Github: https://github.com/renancaldas\n- Facebook: https://www.facebook.com/renan.caldas.oliveira","created":"2016-04-20T23:51:50.200Z","modified":"2016-07-06T16:11:32.224Z","lastPublisher":{"name":"renanzeira","email":"renan.caldas@outlook.com"},"owners":[{"name":"renanzeira","email":"renan.caldas@outlook.com"}],"other":{"_attachments":{},"_from":".","_id":"scrapr","_nodeVersion":"6.2.1","_npmOperationalInternal":{"host":"packages-16-east.internal.npmjs.com","tmp":"tmp/scrapr-0.0.15.tgz_1467821491174_0.45049766451120377"},"_npmUser":{"name":"renanzeira","email":"renan.caldas@outlook.com"},"_npmVersion":"3.9.3","_rev":"1-84c58a063f9ad26ec67ddca25018cbd3","_shasum":"fb8b860b39798fbf46905170276ac131d0e39dc2","author":{"name":"Renan Caldas","email":"renan.calda@gmail.com"},"bugs":{"url":"https://github.com/renancaldas/scrapr/issues"},"directories":{},"dist-tags":{"latest":"0.0.15"},"dist":{"shasum":"fb8b860b39798fbf46905170276ac131d0e39dc2","tarball":"http://registry.npmjs.org/scrapr/-/scrapr-0.0.15.tgz"},"maintainers":[{"name":"renanzeira","email":"renan.caldas@outlook.com"}],"readmeFilename":"README.md","time":{"modified":"2016-07-06T16:11:32.224Z","created":"2016-04-20T23:51:50.200Z","0.0.1":"2016-04-20T23:51:50.200Z","0.0.2":"2016-04-20T23:52:18.942Z","0.0.3":"2016-04-21T00:27:09.406Z","0.0.4":"2016-04-21T01:05:01.593Z","0.0.5":"2016-04-21T01:11:01.130Z","0.0.6":"2016-04-21T01:42:13.088Z","0.0.7":"2016-04-26T12:29:55.635Z","0.0.8":"2016-04-27T15:58:03.964Z","0.0.9":"2016-04-27T16:17:10.178Z","0.0.10":"2016-04-27T16:17:52.684Z","0.0.11":"2016-05-04T19:52:10.011Z","0.0.12":"2016-05-04T19:57:02.204Z","0.0.13":"2016-06-08T04:28:09.371Z","0.0.14":"2016-06-08T04:36:51.932Z","0.0.15":"2016-07-06T16:11:32.224Z"}}}