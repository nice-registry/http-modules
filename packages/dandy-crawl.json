{"name":"dandy-crawl","version":"0.1.0","description":"A website crawler. Stays in the same website and return every internal linked URL with the associated HTTP status code","main":"index.js","homepage":"https://github.com/dandy-seo/dandy-crawl","repository":"https://github.com/dandy-seo/dandy-crawl","keywords":["dandyseo","dandy","seo","crawler","spider"],"license":"GPL-3.0","dependencies":{"cheerio":"^0.20.0","lodash":"^4.10.0","request-promise":"^2.0.1","valid-url":"^1.0.9"},"gitHead":"cd0298550efe499d471e213fb4ee10c1ed423f3a","scripts":{},"versions":[{"number":"0.1.0","date":"2016-04-13T21:15:53.345Z"}],"readme":"# Dandy Crawl\n\nCrawl the interwebs like a real Dandy ಠ_ರೃ ! \n\n![npm version](https://nodei.co/npm/dandy-crawl.png)\n\nA website crawler, that return every internal URLs with the associated HTTP status code. Returned data is an oriented graph, following this model :\n\n```javascript\nnodes: {\n    values: [],\n    lastNodeId: 0,\n    push(currentUrl) {\n        this.values.push({\n            id: this.lastNodeId,\n            url: currentUrl,\n            isExplored: false,\n        });\n        this.lastNodeId++;\n    },\n    get(currentUrl) {\n        return this.values.filter(function (node) {\n            return node.url === currentUrl;\n        })[0]\n    },\n},\nedges: {\n    values: [],\n    lastEdgeId: 0,\n    push(from, to) {\n        this.values.push({\n            id: this.lastEdgeId,\n            from,\n            to,\n        });\n        this.lastEdgeId++;\n    },\n    get(from, to) {\n        return this.values.filter(function (edge) {\n            return edge.from === from && edge.to === to;\n        })[0]\n    },\n}\n```\n\n## Usage\n\n```javascript\nif (!process.argv[2]) {\n    throw \"Argument 1 should be an url\";\n}\n\nconst mod = require('./'),\n      url = process.argv[2];\n\nmod.exploreDomain(url)\n  .then(function (data) {\n    console.log(data.nodes.values);\n    console.log(data.edges.values);\n  })\n  .catch(function(e){\n    console.log(e);\n  });\n```","created":"2016-04-13T21:15:53.345Z","modified":"2016-04-13T21:15:53.345Z","lastPublisher":{"name":"tmos","email":"contact@tomcanac.com"},"owners":[{"name":"tmos","email":"contact@tomcanac.com"}],"other":{"_attachments":{},"_from":".","_id":"dandy-crawl","_nodeVersion":"5.5.0","_npmOperationalInternal":{"host":"packages-16-east.internal.npmjs.com","tmp":"tmp/dandy-crawl-0.1.0.tgz_1460582151662_0.5180332148447633"},"_npmUser":{"name":"tmos","email":"contact@tomcanac.com"},"_npmVersion":"3.7.2","_rev":"1-2be2748fb55804c27f8d81f1164b8eff","_shasum":"b6528b23588421d605535e4edfc62afb45477236","author":{"name":"Tom Canac","email":"contact@tomcanac.com","url":"http://tomcanac.com"},"bugs":{"url":"https://github.com/dandy-seo/dandy-crawl/issues"},"directories":{},"dist-tags":{"latest":"0.1.0"},"dist":{"shasum":"b6528b23588421d605535e4edfc62afb45477236","tarball":"http://registry.npmjs.org/dandy-crawl/-/dandy-crawl-0.1.0.tgz"},"maintainers":[{"name":"tmos","email":"contact@tomcanac.com"}],"readmeFilename":"README.md","time":{"modified":"2016-04-13T21:15:53.345Z","created":"2016-04-13T21:15:53.345Z","0.1.0":"2016-04-13T21:15:53.345Z"}}}