{"name":"latency-header-poc","description":"Proof of concept for using HTTP headers to benchmark latency","scripts":{"client":"node client.js","prestart":"npm install","start":"npm run-script test","pretest":"node --harmony server.js &","test":"node client.js","posttest":"pkill latencyPoC"},"version":"0.1.2","license":"ISC","dependencies":{"koa":"^0.12.2","unirest":"^0.2.7"},"gitHead":"fa2d478db0d8d1039787557da48fde4c7bec99f4","versions":[{"number":"0.1.2","date":"2014-11-18T19:58:05.572Z"}],"readme":"# Latency Headers PoC\n\nAdding HTTP headers to http responses enables developers to gain a better view into their API layer's latency and networking bottlenecks.\n\n### Explaination\n\nBasically there are two headers introduced in this proof of concept:\n\n- `x-request-received` is set by the server with the timestamp of when the request was received\n- `x-response-sent` is set by the server with the timestamp of when the response was sent\n\nWith these headers in place the client can then determine the following:\n\n- **outgoing network latency**: time between client sending the request and server receiving it\n- **server processing latency**: time between server receiving the request and sending the response\n- **incoming network latency**: time between server sending the response and client receiving it\n- **total round trip latency**: time between client sending the request and receiving response\n\n### Complete Example\n\n#### server.js\n\n```js\n// Load the http module\nvar http = require('http');\n\n// Make it easy to pkill from npm\nprocess.title = \"latencyServer\"\n\n// Create the server\nvar server = http.createServer(function (request, response) {\n\n  // Set the x-request-received header with the current timestamp\n  response.setHeader('x-request-received', new Date().getTime());\n\n  // Simulate a delay for processing latency to show up\n  setTimeout(function(){\n\n    // Set the x-response-sent header with the current timestamp\n    response.setHeader('x-response-sent', new Date().getTime());\n\n    // Set the header status to ok and the content type\n    response.writeHead(200, {\"Content-Type\": \"text/plain\"});\n\n    // Return the demobligatory hello world response\n    response.end(\"Hello World\\n\");\n\n  // Remember that delay? 50-75ms sounds like a good target\n  }, Math.floor((Math.random() * 25) + 50))\n\n});\n\n// Listen on localhost:1337\nserver.listen(1337);\n```\n\n#### app.js\n\n```js\n// Require http for requests\nvar http = require('http')\n\n// Save the timestamp of when the request was sent as its required\nvar requestSent = new Date().getTime()\n\n// Send the request to a server that returns the latency headers\nhttp.get(\"http://localhost:1337\", function(res) {\n\n  // Save the response headers for easy access later\n  var headers = res.headers\n\n  // Save the timestamp of when the response was received\n  var responseReceived = new Date().getTime()\n\n  // Save the headers we care about in variables\n  var responseSent = headers['x-response-sent'] || false\n  var requestReceived = headers['x-request-received'] || false\n\n  // If the latency headers do not exist\n  if (!responseSent || !requestReceived){\n    throw new Error(\"The server did not respond with latency headers.\")\n  }\n\n  // The math to determine the latencies\n  var outgoingLatency = requestReceived - requestSent\n  var processingLatency = responseSent - requestReceived\n  var incomingLatency = responseReceived - responseSent\n  var roundtripLatency = outgoingLatency + processingLatency + incomingLatency\n\n  // Return back the latencies\n  var results = {\n      \"outgoingLatency\" : outgoingLatency,\n      \"processingLatency\" : processingLatency,\n      \"incomingLatency\" : incomingLatency,\n      \"roundtripLatency\" : roundtripLatency\n  }\n\n  // Output the results\n  console.log(results)\n})\n```\n\n### Try it out\n\nAn example server and app is included in this repo for your convenience. \n\n```shell\n# Clone this repo\ngit clone git@github.com:montanaflynn/latency-header-benchmark.git\n\n# CD to the example dir\ncd latency-header-benchmark/example\n\n# Install this package (npmception?)\nnpm install\n\n# Start the server in the background\nnode server.js &\n\n# Run the app\nnode app.js\n\n# Kill the example server\npkill latencyServer\n```\n\n### Related projects\n\n- [Latency Header Benchmarker](https://github.com/montanaflynn/latency-header-benchmark)\n- [Express Latency Header Middleware](https://github.com/montanaflynn/express-latency-headers)\n- [Koa Latency Header Middleware](https://github.com/montanaflynn/koa-latency-headers)\n\nCopyright (c) 2014, Montana Flynn (http://anonfunction.com/)\n","created":"2014-11-18T19:58:05.572Z","modified":"2014-11-18T19:58:05.572Z","lastPublisher":{"name":"montanaflynn","email":"montana@montanaflynn.me"},"owners":[{"name":"montanaflynn","email":"montana@montanaflynn.me"}],"other":{"_attachments":{},"_from":".","_id":"latency-header-poc","_nodeVersion":"0.11.13","_npmUser":{"name":"montanaflynn","email":"montana@montanaflynn.me"},"_npmVersion":"2.1.8","_rev":"1-2dc5dc5f3497bb598cd65466c12283cf","_shasum":"10919c63dec44346750e1d12b6586fcde9bb896b","author":{"name":"Montana Flynn"},"directories":{},"dist-tags":{"latest":"0.1.2"},"dist":{"shasum":"10919c63dec44346750e1d12b6586fcde9bb896b","tarball":"http://registry.npmjs.org/latency-header-poc/-/latency-header-poc-0.1.2.tgz"},"maintainers":[{"name":"montanaflynn","email":"montana@montanaflynn.me"}],"readmeFilename":"README.md","time":{"modified":"2014-11-18T19:58:05.572Z","created":"2014-11-18T19:58:05.572Z","0.1.2":"2014-11-18T19:58:05.572Z"}}}